{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import model_from_json, load_model\n",
    "import numpy\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import pyipopt\n",
    "from numpy import *\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################### Simulation time step ##################################\n",
    "delta = 0.01   # sampling time\n",
    "hc = 1e-4     # integration time step\n",
    "oper_time = 0.01  \n",
    "\n",
    "################################## Initial states #########################################\n",
    "CAi=-1.65\n",
    "Ti=72\n",
    "x1_nn=CAi\n",
    "x2_nn=Ti\n",
    "x1_record=[CAi]\n",
    "x2_record=[Ti]\n",
    "u1_record=[]\n",
    "u2_record=[]\n",
    "\n",
    "##################################### Constants ###########################################\n",
    "a=1060\n",
    "b=22\n",
    "d=0.52  # Lyapunov function V=x^T*P*x\n",
    "\n",
    "# CSTR PARAMETERS\n",
    "F=5\n",
    "V=1\n",
    "k0=8460000\n",
    "E=50000     # parametric drift #####E has the most effect on process dynamics######\n",
    "R=8.314\n",
    "T0=300\n",
    "Dh=-11500\n",
    "rho=1000\n",
    "sigma=1000\n",
    "cp=0.231\n",
    "Qs=0\n",
    "CA0s=4\n",
    "w1_std=2.5  #disturbance std\n",
    "w2_std=70   #disturbance std\n",
    "\n",
    "\n",
    "########################################### steady-state ###################################\n",
    "CAs= 1.9537\n",
    "Ts=  401.8727\n",
    "\n",
    "# state_ss=numpy.array([Ts, CAs])\n",
    "# input_ss=numpy.array([Qs, CA0s])\n",
    "state_ss=numpy.array([CAs,Ts])\n",
    "input_ss=numpy.array([CA0s,Qs])\n",
    "\n",
    "ROOT_FOLDER=os.getcwd()\n",
    "\n",
    "##################################### MPC Parameters ######################################\n",
    "\n",
    "NUM_MPC_ITERATION=10   # MPC TOTAL ITERATION\n",
    "\n",
    "\n",
    "NUM_OUTPUTS = 2 # Number of state variables (RNN output)  \n",
    "NUM_INPUTS = 4  # Number of RNN input\n",
    "HORIZON = 2   ## MPC PREDICTION HORIZON (Depends on how many delta you want to predict with the RNN)\n",
    "\n",
    " \n",
    "NUM_IN_SEQUENCE = 10  # Number of integration time steps actually used in MPC (equal to timestep of ML model)\n",
    "PREDICTION_STORE = 0   ## \n",
    "\n",
    "NUM_MPC_INPUTS = 2*HORIZON  # Number of MPC inputs to be optimized, 2 is the number of control inputs \n",
    "NUM_MPC_CONSTRAINTS = HORIZON  # For each sampling time within prediction horizon, we have 1 constraint\n",
    "\n",
    "realtime_data = None\n",
    "\n",
    "setpoint=[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scalers for both X and y\n",
    "X_mean = np.load('X_mean.npy')\n",
    "X_std = np.load('X_std.npy')\n",
    "y_mean = np.load('y_mean.npy')\n",
    "y_std = np.load('y_std.npy')\n",
    "\n",
    "model = load_model('model1.h5')\n",
    "print(model)\n",
    "\n",
    "x1_mean= X_mean[0]   # CA\n",
    "x1_std= X_std[0]\n",
    "x2_mean=X_mean[1]  # T\n",
    "x2_std= X_std[1]\n",
    "u1_mean= X_mean[2]    # CA0\n",
    "u1_std=X_std[2]\n",
    "u2_mean=X_mean[3]     # Q\n",
    "u2_std=X_std[3]\n",
    "y1_mean=y_mean[0]   # CA\n",
    "y1_std=y_std[0]\n",
    "y2_mean=y_mean[1]   # T\n",
    "y2_std=y_std[1]\n",
    "\n",
    "state_mean = np.array([x1_mean, x2_mean])  # [CA_input, T_input]\n",
    "state_std = np.array([x1_std, x2_std])\n",
    "\n",
    "input_mean = np.array([u1_mean, u2_mean])  # [CA0, Q]\n",
    "input_std = np.array([u1_std, u2_std])\n",
    "\n",
    "output_mean = np.array([y1_mean, y2_mean])  # [CA_output, T_output]\n",
    "output_std = np.array([y1_std, y2_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ens_prediction(num_horizon,my_rawdata,my_inputs):\n",
    "    xx = []  \n",
    "    nn_inputs = []  # NN input\n",
    "    ensemble_output = numpy.zeros((num_horizon,NUM_OUTPUTS,NUM_IN_SEQUENCE))\n",
    "    \n",
    "    ensemble_output = ensemble_output.reshape(num_horizon,NUM_IN_SEQUENCE,NUM_OUTPUTS)\n",
    "    x_test2 = my_rawdata[0:NUM_OUTPUTS].astype(float)\n",
    "    x_test2= (x_test2-state_mean)/state_std           # my_rawdata is normal value; needs to normalize before feeding into NN\n",
    "\n",
    "    predict_output_normal=[[0 for i in range(NUM_OUTPUTS)] for j in range(NUM_IN_SEQUENCE)]\n",
    "\n",
    "    for i_model in range(num_horizon):    \n",
    "  \n",
    "        # ############################# get the normalized input for RNN #########################\n",
    "         \n",
    "        my_inputs_normalized = (my_inputs[2*i_model:2*(i_model+1)] - input_mean) / input_std    # my_inputs is also normal value; needs to normalize before feeding into NN\n",
    "        xx = numpy.concatenate((x_test2,  my_inputs_normalized), axis=None).reshape((1, NUM_INPUTS))  # xx is the NN input\n",
    "        xx = numpy.tile(xx, (NUM_IN_SEQUENCE, 1))  # duplicate #NUM_IN_SEQUENCE times\n",
    "\n",
    "        nn_inputs = xx.reshape(1, NUM_IN_SEQUENCE, NUM_INPUTS) \n",
    "        # ############## use machine learning model to predict the next sampling time ##############\n",
    "        \n",
    "        predict_output = model.predict(nn_inputs)\n",
    "        predict_output = predict_output.reshape(NUM_IN_SEQUENCE, NUM_OUTPUTS)\n",
    "        predict_output = predict_output * output_std + output_mean  # convert back to deviation form\n",
    "\n",
    "        ####################### get the system state at the end of sampling time ################\n",
    "        \n",
    "        x_test2 = predict_output[-1, :]\n",
    "\n",
    "\n",
    "        ###### transform the predicted states back to their original values in deviation form ######\n",
    "        \n",
    "        x_test2 = (x_test2 - state_mean) / state_std  # normalize input for next prediction\n",
    "\n",
    "        ensemble_output[i_model,:,:] = predict_output[:, :] # store the predicted states \n",
    "\n",
    "        ########################################################################################\n",
    "\n",
    "\n",
    "    return ensemble_output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_f(x):\n",
    "    '''\n",
    "    define the objective function\n",
    "    '''\n",
    "    assert len(x) == int(NUM_MPC_INPUTS)\n",
    "    offset=0\n",
    "    # global PREDICTION_STORE\n",
    "    #### CALCULATE OUTLET CONC ###########\n",
    "    df_ensemble_output = my_ens_prediction(num_horizon = HORIZON, my_rawdata=realtime_data, my_inputs=x)\n",
    "    # LAST SUBENSEMBLE, LAST TIME STEP, FIRST VARIABLE\n",
    "    # factor=realtime_data[1] **2 *500 /(realtime_data[0] **2 *50)\n",
    "    # factor2=realtime_data[1] **2 *500 /(3.5 **2 *100)\n",
    "    #### account for all intermediate steps ####\n",
    "    for j in range (HORIZON):\n",
    "        est_outlet_product = df_ensemble_output[j, :, 0:2]\n",
    "        for i in range (int(NUM_IN_SEQUENCE)):  \n",
    "           \n",
    "             offset = offset + (setpoint[0] - (est_outlet_product[i, 0])) ** 2.0  + (setpoint[1] - (est_outlet_product[i, 1])) ** 2.0 * 1000\n",
    "        offset = offset+x[2*j] **2 *3e-10 + 1* x[2*j+1] **2\n",
    "        #offset=offset+(x[1]-x_record[1])**2 *factor2/10+(x[0]-x_record[0])**2 *factor2/1e11\n",
    "\n",
    "    return offset/100\n",
    "\n",
    "def eval_grad_f(x):\n",
    "    '''\n",
    "    define the gradient of the objective function\n",
    "    '''\n",
    "    assert len(x) == int(NUM_MPC_INPUTS)\n",
    "    step = 1e-1 # we just have a small step\n",
    "    objp=objm=0\n",
    "    grad_f = [0]*NUM_MPC_INPUTS\n",
    "    xpstep = [0]*NUM_MPC_INPUTS\n",
    "    xmstep = [0]*NUM_MPC_INPUTS\n",
    "    for i_mpc_input in range(NUM_MPC_INPUTS):\n",
    "        xpstep=x.copy()\n",
    "        xmstep=x.copy()\n",
    "        # for each variables, we need to evaluate the derivative of the function with respect to that variable, This is why we have the for loop\n",
    "        xpstep[i_mpc_input]  = xpstep[i_mpc_input]+step \n",
    "        xmstep[i_mpc_input] = xmstep[i_mpc_input]-step\n",
    "\n",
    "        # Evaluate the objective function at xpstep and xmstep\n",
    "        objp=eval_f(xpstep) # This function returns the value of the objective function evaluated with the variable x[i] is perturebed +step\n",
    "        objm=eval_f(xmstep) # This function returns the value of the objective function evaluated with the variable x[i] is perturebed -step\n",
    "        #print (\"obj \", objp, \"   objm   \", objm)\n",
    "        grad_f[i_mpc_input] = (objp - objm) / (2 * step) # This evaluates the gradient of the objetive function with repect to the optimization variable x[i]\n",
    "    #print(\"Gradient: \", grad_f)\n",
    "    return array(grad_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_g(x):\n",
    "    '''\n",
    "    define the constraint function\n",
    "    '''\n",
    "    assert len(x) == int(NUM_MPC_INPUTS)\n",
    "    #### CALCULATE FLUID TEMPERATURE ALONG THE FIRST THREE SURFACES ###########\n",
    "    \n",
    "    CAd2=realtime_data[0]\n",
    "    Td2=realtime_data[1]\n",
    "\n",
    "    g=array([-5.0]*NUM_MPC_CONSTRAINTS)\n",
    "\n",
    "    return  g\n",
    "\n",
    "nnzj = NUM_MPC_CONSTRAINTS*NUM_MPC_INPUTS\n",
    "\n",
    "\n",
    "def eval_jac_g(x, flag):\n",
    "    '''\n",
    "    define the Jacobian of the constraint function\n",
    "    '''\n",
    "    \n",
    "    if flag:\n",
    "        list_x = []\n",
    "        list_y=[]\n",
    "        for i in range(int(NUM_MPC_INPUTS / 2)):\n",
    "            list_x = list_x + [i] * NUM_MPC_INPUTS\n",
    "            list_y = list_y +list(range(0, int(NUM_MPC_INPUTS)))\n",
    "\n",
    "        return (array(list_x),\n",
    "                array(list_y))\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert len(x) == int(NUM_MPC_INPUTS)\n",
    "        step = 1e-1 # we just have a small step\n",
    "        gp=gm=numpy.zeros(NUM_MPC_CONSTRAINTS)\n",
    "        xpstep=xmstep=numpy.zeros(NUM_MPC_INPUTS)\n",
    "        jac_g = [[0]*int(NUM_MPC_INPUTS) for _ in range(NUM_MPC_CONSTRAINTS)]\n",
    "\n",
    "        for i_mpc_input in range(NUM_MPC_INPUTS):\n",
    "            xpstep=x.copy()\n",
    "            xmstep=x.copy()\n",
    "            # for each variables, we need to evaluate the derivative of the function with respect to that variable, This is why we have the for loop\n",
    "            xpstep[i_mpc_input] += step \n",
    "            xmstep[i_mpc_input] -= step\n",
    "            gp=eval_g(xpstep)\n",
    "            gm=eval_g(xmstep)\n",
    "            for num_constraint in range(NUM_MPC_CONSTRAINTS):\n",
    "                jac_g[num_constraint][i_mpc_input] = (gp[num_constraint] - gm[num_constraint]) / (2 * step)\n",
    "            #print (\"in eval_jac_g_2:\")\n",
    "        return array(jac_g)\n",
    "\n",
    "def apply_new(x):\n",
    "    return True\n",
    "def print_variable(variable_name, value):\n",
    "    for i in range(len(value)):\n",
    "        print(\"{} {}\".format(variable_name + \"[\"+str(i)+\"] =\", value[i]))\n",
    "\n",
    "\n",
    "nnzh = NUM_MPC_INPUTS**2  ## number of nonzeros in the Hessian of the Lagrangian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = NUM_MPC_INPUTS   ## number of variables\n",
    "x_lower=[0]* nvar   ## lower bound of the variables\n",
    "x_upper=[0]* nvar  ## upper bound of the variables  \n",
    "for i in range(int(HORIZON)):\n",
    "    x_lower[2*i]= -3.5 \n",
    "    x_lower[2 * i+1] = -5e5\n",
    "    x_upper[2 * i] = 3.5\n",
    "    x_upper[2 * i + 1] = 5e5\n",
    "x_L = array(x_lower) #array([-3.5,-5e5])\n",
    "x_U = array(x_upper) #array([3.5, 5e5])\n",
    "\n",
    "### DEFINE THE UPPER BOUND AND LOWER BOUND OF THE CONSTRAINT ###\n",
    "ncon = NUM_MPC_CONSTRAINTS      ## number of constraints\n",
    "g_L = array([-2e19]*HORIZON)    ## lower bound of the constraints\n",
    "g_U = array([0]*HORIZON)        ## upper bound of the constraints\n",
    "\n",
    "print (\"g_L\", g_L, g_U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_iteration in range(NUM_MPC_ITERATION):\n",
    "    print (\"Num Iteratin: \", main_iteration)\n",
    "\n",
    "    rawdata = numpy.array([CAi, Ti])\n",
    "    \n",
    "    realtime_data=rawdata\n",
    "\n",
    "    nlp = pyipopt.create(nvar, x_L, x_U, ncon, g_L, g_U, nnzj, nnzh, eval_f, eval_grad_f, eval_g, eval_jac_g)\n",
    "\n",
    "    if main_iteration ==0 :\n",
    "        x0 = array([0.0]*int(NUM_MPC_INPUTS))   # initial guess\n",
    "      \n",
    "    else:\n",
    "        x0=x    # use the previous solution as the initial guess\n",
    "        x0[0:-2]=x[2:]  # shift the previous solution to the left by 2\n",
    "        x0[-2:]=x[-2:]  # keep the last two elements unchanged\n",
    "        x_record=x\n",
    "\n",
    "    \"\"\"\n",
    "    print x0\n",
    "    print nvar, ncon, nnzj\n",
    "    print x_L,  x_U\n",
    "    print g_L, g_U\n",
    "    print eval_f(x0)\n",
    "    print eval_grad_f(x0)\n",
    "    print eval_g(x0)\n",
    "    a =  eval_jac_g(x0, True)\n",
    "    print \"a = \", a[1], a[0]\n",
    "    print eval_jac_g(x0, False)\n",
    "    print eval_h(x0, pi0, 1.0, False)\n",
    "    print eval_h(x0, pi0, 1.0, True)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You CAd2 set Ipopt options by calling nlp.num_option, nlp.str_option\n",
    "    or nlp.int_option. For instance, to set the tolarance by calling\n",
    "\n",
    "        nlp.num_option('tol', 1e-8)\n",
    "\n",
    "    For a complete list of Ipopt options, refer to\n",
    "\n",
    "        http://www.coin-or.org/Ipopt/documentation/node59.html\n",
    "\n",
    "    Note that Ipopt distinguishs between Int, Num, and Str options, yet sometimes\n",
    "    does not explicitly tell you which option is which.  If you are not sure about\n",
    "    the option's type, just try it in PyIpopt.  If you try to set one type of\n",
    "    option using the wrong function, Pyipopt will remind you of it. \"\"\"\n",
    "\n",
    "    nlp.int_option('max_iter', 1000)    # maximum number of iterations\n",
    "    nlp.num_option('tol', 1e-5)         # convergence tolerance\n",
    "    nlp.int_option('print_level', 2)    # print out the process\n",
    "    print(\"Going to call solve\")        # solve the problem\n",
    "    print(\"x0 = {}\".format(x0))         # initial guess\n",
    "    x, zl, zu, constraint_multipliers, obj, status = nlp.solve(x0)  \n",
    "\n",
    "    nlp.close()\n",
    "\n",
    "    print(\"Solution of the primal variables, x\")\n",
    "    print_variable(\"x\", x)\n",
    "    print (\"status=\", status)\n",
    "\n",
    "    print(\"Objective value\")\n",
    "    print(\"f(x*) = {}\".format(obj))\n",
    "    print (\"Control action=:  \", x[0], x[1])\n",
    "\n",
    "    x1=CAi  \n",
    "    x2=Ti  \n",
    "\n",
    "    # w is the disturbance\n",
    "    w1 =numpy.random.normal(0, w1_std, 1)\n",
    "    w2 =numpy.random.normal(0, w2_std, 1)\n",
    "    if w1>w1_std:\n",
    "        w1=w1_std\n",
    "    if w1<-w1_std:\n",
    "        w1=-w1_std\n",
    "    if w2>w2_std:\n",
    "        w2=w2_std\n",
    "    if w2>w2_std:\n",
    "        w2=w2_std\n",
    "    #print (numpy.asscalar(w1))\n",
    "    #print (numpy.asscalar(w2))\n",
    "    for kk in range (int(delta/hc)):    # apply the control action for real model\n",
    "\n",
    "\n",
    "        x1_new = x1 + hc * ((F / V) * (x[0] - x1) -\n",
    "                            k0 * ((numpy.exp(-E / (R * (x2 + Ts)))*(x1 + CAs) * (x1 + CAs))\n",
    "                                  - numpy.exp(-E / (R * Ts)) * CAs * CAs)+5*float(w1))\n",
    "\n",
    "        x2_new = x2 + hc * (((F / V) * (-x2) + (-Dh / (sigma * cp)) *\n",
    "                             (k0 * ((numpy.exp(-E / (R * (x2 + Ts))) * (x1 + CAs) * (x1 + CAs)) -\n",
    "                                      numpy.exp(-E / (R * Ts)) * CAs * CAs)) + (x[1] / (sigma * cp * V)))+5*float(w2))\n",
    "\n",
    "\n",
    "\n",
    "        x1 = x1_new\n",
    "        x2 = x2_new\n",
    "\n",
    "        # if (kk%5==1):\n",
    "        #     x1_record.append(x1)\n",
    "        #     x2_record.append(x2)\n",
    "        #     u1_record.append(x[1])\n",
    "        #     u2_record.append(x[0])\n",
    "\n",
    "    CAi=x1\n",
    "    Ti=x2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Real model output x1 x2 in deviation form:   ', x1, x2)\n",
    "\n",
    "    x1_record.append(x1)\n",
    "    x2_record.append(x2)\n",
    "    u1_record.append(x[0])\n",
    "    u2_record.append(x[1])\n",
    "\n",
    "print (\"x1_record: \",x1_record)\n",
    "print (\"x2_record: \",x2_record)\n",
    "\n",
    "print (\"u1_record: \",u1_record)\n",
    "print (\"u2_record: \",u2_record)\n",
    "\n",
    "# numpy.savetxt(\"x1.txt\",   x1_record, fmt=\"%f\",  delimiter=\" \")\n",
    "# numpy.savetxt(\"x2.txt\",   x2_record, fmt=\"%f\",  delimiter=\" \")\n",
    "\n",
    "\n",
    "# numpy.savetxt(\"u1.txt\",   u1_record, fmt=\"%f\",  delimiter=\" \")\n",
    "# numpy.savetxt(\"u2.txt\",   u2_record, fmt=\"%f\",  delimiter=\" \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
